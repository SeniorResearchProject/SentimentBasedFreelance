{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgmbKtDekugL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_mn8fRCk0Ff",
        "outputId": "ab820207-0450-4684-f844-47960b73adb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "import pandas as pd\n",
        "\n",
        "# Update the path below based on where the file is located in your Google Drive\n",
        "file_path = '/content/drive/My Drive/amharic.xlsx'\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltyV4s1bk1ae",
        "outputId": "556ac7c3-f5fa-4ada-896b-dcd156c45ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Comments Sentiment Unnamed: 2  \\\n",
            "0  እባካችሁ በትግራይ ክልል 2ኛ እና 3ኛ ዲግሪ ሲማሩ ለቆዩ ተማሪዎች ጉዳይ...         0        NaN   \n",
            "1                          ነጭ ሩዝ በምስር? ለፍየል የማይሰጥ ፋፋ         2        NaN   \n",
            "2          አንተ ልዩ ሰዉ ነህ እስከ ህይወት መስዋዕትነት ድረስ ከጎንህ ነን         2        NaN   \n",
            "3                               አንተ ደደብ የሆንክ አፍህን ዝጋ         0        NaN   \n",
            "4  ትምህርት ሚኒስቴር እየሰራ ያለው ሥራ ምንም አልገባኝ አለ በየዓመቱ ኮርስ...         0        NaN   \n",
            "\n",
            "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  \n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN  \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN  \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN  \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN  \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN  \n"
          ]
        }
      ],
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "df = pd.read_excel(file_path)\n",
        "print(df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMIhgAKWlCvM"
      },
      "outputs": [],
      "source": [
        "# Check if each row in the Sentiment column is a string\n",
        "string_rows = df['Sentiment'].apply(lambda x: isinstance(x, str))\n",
        "# Filter out rows where Sentiment is a string\n",
        "cleaned_data = df[~string_rows]\n",
        "df['Sentiment'] = pd.to_numeric(df['Sentiment'], errors='coerce')\n",
        "\n",
        "# Optional: Handle NaN values. Here's an example of dropping them\n",
        "df.dropna(subset=['Sentiment'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEOi0kM7lEMa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Remove rows where the 'sentiment' column is 2 or 3\n",
        "df = df[~df['Sentiment'].isin([2, 3])]\n",
        "\n",
        "# Now df will only contain rows where the sentiment is not equal to 2 or 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si_GI4PMlGyw",
        "outputId": "e573e48d-1519-435c-c7fb-05f4b8c45597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiments\n",
            "negative    3553\n",
            "positive    1788\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = df[['Comments', 'Sentiment']]\n",
        "df['sentiments'] = df['Sentiment'].apply(lambda x: 'positive' if x >= 1 else 'negative')\n",
        "\n",
        "df = df[['Comments', 'sentiments']]\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "sentiment_counts = df['sentiments'].value_counts()\n",
        "print(sentiment_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cNovVcElKH6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df is your DataFrame containing the comments and sentiments\n",
        "# Clean and preprocess the data\n",
        "df['Comments'] = df['Comments'].fillna('')  # Replace NaN with empty string\n",
        "df['Comments'] = df['Comments'].astype(str)  # Ensure all data is string type\n",
        "\n",
        "# Features and target variable\n",
        "X = df['Comments']\n",
        "y = df['sentiments']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataFrame for training and testing\n",
        "df_train = pd.DataFrame({'Comments': X_train, 'sentiments': y_train})\n",
        "df_test = pd.DataFrame({'Comments': X_test, 'sentiments': y_test})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY49p6IYlNkK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(row, options):\n",
        "    \"\"\"Removes url, mentions, emoji, and converts to lowercase if applicable\"\"\"\n",
        "    # Ensure row is a string\n",
        "    if not isinstance(row, str):\n",
        "        # If row is not a string, return it as is or convert it to a string as per your requirement\n",
        "        return row  # Or, return str(row) if you want to convert non-strings to strings\n",
        "\n",
        "    if options['lowercase']:\n",
        "        row = row.lower()\n",
        "\n",
        "    if options['remove_url']:\n",
        "        row = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", row)\n",
        "\n",
        "    if options['remove_mentions']:\n",
        "        row = re.sub(\"@[A-Za-z0-9_]+\",\"\", row)\n",
        "\n",
        "    if options['demojify']:\n",
        "        emoj = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U0001f926-\\U0001f937\"\n",
        "            u\"\\U00010000-\\U0010ffff\"\n",
        "            u\"\\u2640-\\u2642\"\n",
        "            u\"\\u2600-\\u2B55\"\n",
        "            u\"\\u200d\"\n",
        "            u\"\\u23cf\"\n",
        "            u\"\\u23e9\"\n",
        "            u\"\\u231a\"\n",
        "            u\"\\ufe0f\"  # dingbats\n",
        "            u\"\\u3030\"\n",
        "                          \"]+\", re.UNICODE)\n",
        "        row = re.sub(emoj, '', row)\n",
        "\n",
        "    return row\n",
        "clean_config = {\n",
        "    'remove_url': True,\n",
        "    'remove_mentions': True,\n",
        "    'lowercase': True,\n",
        "    'demojify': True\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNuWlTZZlQmw"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_test['Comments'] = df_test['Comments'].apply(clean_text, args=(clean_config,))\n",
        "df_train['sentiments'] = df_train['sentiments'].apply(clean_text, args=(clean_config,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcbpBWO3lUGu",
        "outputId": "7bd2ac08-799d-47d3-dc15-c39cdccf0a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/410.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m327.7/410.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ],
      "source": [
        "pip install nlpaug\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DoBc3oOlYSz"
      },
      "outputs": [],
      "source": [
        "#character level normalization\n",
        "\n",
        "import re\n",
        "#method to normalize character level missmatch such as ጸሀይ and ፀሐይ\n",
        "def normalize_char_level_missmatch(input_token):\n",
        "  # Check if the input is not a string\n",
        "    if not isinstance(input_token, str):\n",
        "        # Return the input as is or convert it to a string, depending on your needs\n",
        "        return input_token  # or return str(input_token) to convert to string\n",
        "    rep1=re.sub('[ሃኅኃሐሓኻ]','ሀ',input_token)\n",
        "    rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
        "    rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
        "    rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
        "    rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
        "    rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
        "    rep7=re.sub('[ሠ]','ሰ',rep6)\n",
        "    rep8=re.sub('[ሡ]','ሱ',rep7)\n",
        "    rep9=re.sub('[ሢ]','ሲ',rep8)\n",
        "    rep10=re.sub('[ሣ]','ሳ',rep9)\n",
        "    rep11=re.sub('[ሤ]','ሴ',rep10)\n",
        "    rep12=re.sub('[ሥ]','ስ',rep11)\n",
        "    rep13=re.sub('[ሦ]','ሶ',rep12)\n",
        "    rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
        "    rep15=re.sub('[ዑ]','ኡ',rep14)\n",
        "    rep16=re.sub('[ዒ]','ኢ',rep15)\n",
        "    rep17=re.sub('[ዔ]','ኤ',rep16)\n",
        "    rep18=re.sub('[ዕ]','እ',rep17)\n",
        "    rep19=re.sub('[ዖ]','ኦ',rep18)\n",
        "    rep20=re.sub('[ጸ]','ፀ',rep19)\n",
        "    rep21=re.sub('[ጹ]','ፁ',rep20)\n",
        "    rep22=re.sub('[ጺ]','ፂ',rep21)\n",
        "    rep23=re.sub('[ጻ]','ፃ',rep22)\n",
        "    rep24=re.sub('[ጼ]','ፄ',rep23)\n",
        "    rep25=re.sub('[ጽ]','ፅ',rep24)\n",
        "    rep26=re.sub('[ጾ]','ፆ',rep25)\n",
        "    #Normalizing words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል\n",
        "    rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
        "    rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
        "    rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
        "    rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
        "    rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
        "    rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
        "    rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
        "    rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
        "    rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
        "    rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
        "    rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
        "    rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
        "    rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
        "    rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
        "    rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
        "    rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
        "    rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
        "    rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
        "    rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
        "    rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
        "    rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
        "    rep48=re.sub('[ኵ]','ኩ',rep47) #ኩ can be also written as ኵ\n",
        "    return rep48\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6POy8I8lcOE"
      },
      "outputs": [],
      "source": [
        "df_test['Comments'] = df_test['Comments'].apply(lambda x: normalize_char_level_missmatch(x))\n",
        "df_train['Comments'] = df_train['Comments'].apply(lambda x: normalize_char_level_missmatch(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsjmTLLglf5C",
        "outputId": "00b34206-2943-497e-d382-53622dd789e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original training data size: 4272\n",
            "Augmented training data size: 17088\n"
          ]
        }
      ],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to augment data\n",
        "def augment_text(data, augmenter, repetitions=1):\n",
        "    augmented_texts = []\n",
        "    for text in data:\n",
        "        # Ensure text is a string\n",
        "        text = str(text)\n",
        "        for _ in range(repetitions):\n",
        "            augmented = augmenter.augment(text)\n",
        "            augmented_texts.append(str(augmented))  # Ensure the augmented text is a string\n",
        "    return augmented_texts\n",
        "\n",
        "# Assuming df is already loaded and contains the columns 'Review' and 'Liked'\n",
        "# Split your data\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['Liked'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize augmenter - here using synonym replacement\n",
        "augmenter = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "# Augment data - now tripling the data\n",
        "augmented_texts = augment_text(X_train, augmenter, repetitions=3)  # Tripling data with augmentation\n",
        "\n",
        "# Reset index for y_train to properly repeat it\n",
        "y_train_reset = y_train.reset_index(drop=True)\n",
        "\n",
        "# Extend the original training data with augmented data\n",
        "augmented_data = pd.DataFrame({'Comments': augmented_texts, 'sentiments': y_train_reset.repeat(3).values})  # Repeat y_train three times\n",
        "X_train_augmented = pd.concat([X_train, augmented_data['Comments']]).reset_index(drop=True)\n",
        "y_train_augmented = pd.concat([y_train, augmented_data['sentiments']]).reset_index(drop=True)\n",
        "\n",
        "# Verification that all entries are strings\n",
        "if any(not isinstance(x, str) for x in X_train_augmented):\n",
        "    print(\"Non-string elements detected in augmented data.\")\n",
        "\n",
        "assert all(isinstance(x, str) for x in X_train_augmented), \"All elements in X_train_augmented must be strings.\"\n",
        "\n",
        "# Now you have a significantly augmented dataset\n",
        "print(\"Original training data size:\", len(X_train))\n",
        "print(\"Augmented training data size:\", len(X_train_augmented))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BakarMlalrb_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_imblearn\n",
        "\n",
        "class StackingSentiment:\n",
        "    def __init__(self):\n",
        "        # Define base models with their respective pipelines\n",
        "        base_models = [\n",
        "            ('naive_bayes', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5),\n",
        "                SMOTE(),\n",
        "                MultinomialNB(alpha=0.1)\n",
        "            )),\n",
        "            ('svm', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5),\n",
        "                SMOTE(),\n",
        "                SVC(probability=True, kernel='linear', C=1)\n",
        "            )),\n",
        "            ('random_forest', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5),\n",
        "                SMOTE(),\n",
        "                RandomForestClassifier(n_estimators=100)\n",
        "            )),\n",
        "            ('log_reg', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5),\n",
        "                SMOTE(),\n",
        "                LogisticRegression(C=1)\n",
        "            ))\n",
        "        ]\n",
        "\n",
        "        # Define the final estimator\n",
        "        final_estimator = LogisticRegression()\n",
        "\n",
        "        # Define the stacking classifier with cross-validation\n",
        "        self.pipeline = StackingClassifier(\n",
        "            estimators=base_models,\n",
        "            final_estimator=final_estimator,\n",
        "            cv=5\n",
        "        )\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.pipeline.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.pipeline.predict_proba(X)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assume X_train, X_test, y_train, y_test are predefined\n",
        "\n",
        "    stacking_sentiment = StackingSentiment()\n",
        "    stacking_sentiment.fit(X_train, y_train)  # Train the model\n",
        "    predictions = stacking_sentiment.predict(X_test)  # Predict test data\n",
        "    accuracy = accuracy_score(y_test, predictions)  # Calculate accuracy\n",
        "    print(f\"Accuracy of the stacking classifier: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7egA0j8ahVBU",
        "outputId": "72d48595-314c-4adf-b892-937fca7581c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRAXNdJPgEt1",
        "outputId": "18265c99-98b6-4b77-cf85-fea2595c7dca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the stacking classifier: 0.86\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as make_pipeline_imblearn\n",
        "\n",
        "# Define Amharic stop words list\n",
        "amharic_stop_words = [\n",
        "    \"ህ-ን\", \"እንደ\", \"የ\", \"አል\", \"ው\", \"ኡ\", \"በ\", \"ተ\", \"ለ\", \"ን\", \"ኦች\", \"ኧ\", \"ና\", \"ከ\", \"አቸው\", \"ት\", \"መ\",\n",
        "    \"አ\", \"አት\", \"ዎች\", \"ም\", \"አስ\", \"ኡት\", \"ላ\", \"ይ\", \"ማ\", \"ያ\", \"አ\", \"ቶ\", \"እንዲ\", \"የሚ\", \"ኦ\", \"ይ\", \"እየ\",\n",
        "    \"ሲ\", \"ብ\", \"ወደ\", \"ሌላ\", \"ጋር\", \"ኡ\", \"እዚህ\", \"አንድ\", \"ውስጥ\", \"እንድ\", \"እ-ል\", \"ን-ብ-ር\", \"በኩል\", \"ል\",\n",
        "    \"እስከ\", \"እና\", \"ድ-ግ-ም\", \"መካከል\", \"ኧት\", \"ሊ\", \"አይ\", \"ምክንያት\", \"ይህ\", \"ኧች\", \"ኢት\", \"ዋና\", \"አን\",\n",
        "    \"እየ\", \"ስለ\", \"ች\", \"ስ\", \"ቢ\", \"ብቻ\", \"በየ\", \"ባለ\", \"ጋራ\", \"ኋላ\", \"እነ\", \"አም\", \"ሽ\", \"አዊ\", \"ዋ\", \"ያለ\",\n",
        "    \"ግን\", \"ምን\", \"አችን\", \"ወይዘሮ\", \"ወዲህ\", \"ማን\", \"ዘንድ\", \"የት\", \"ናቸው\", \"ላ\", \"ይሁን\", \"ወይም\", \"ታች\",\n",
        "    \"እዚያ\", \"እጅግ\", \"እንጅ\", \"በጣም\", \"ወዘተ\", \"ጅ-ም-ር\", \"አሁን\", \"ከነ\", \"ተራ\", \"ም-ል\", \"ጎሽ\", \"አዎ\", \"እሽ\",\n",
        "    \"ጉዳይ\", \"ረገድ\", \"ያህል\", \"ይልቅ\", \"ዳር\", \"እንኳ\", \"አዎን\", \"ብ-ዝ\", \"ጥቂት\", \"እኔ\", \"አንተ\", \"እርስዎ\",\n",
        "    \"እሳቸው\", \"እሱ\", \"አንች\", \"እኛ\", \"እነሱ\", \"እናንተ\", \"ይኸ\", \"የቱ\", \"መቼ\", \"ወዲያ\", \"ወዴት\", \"እንዴት\",\n",
        "    \"ልክ\", \"አጠገብ\", \"ባሻገር\", \"እንትን\", \"እንትና\", \"ሁሉ\", \"እንጂ\", \"ይች\", \"ናት\", \"ምናልባት\", \"በቀር\", \"እስኪ\",\n",
        "    \"ወይ\", \"እንዴ\", \"ስንት\", \"መቸ\", \"ከፍ\", \"ቢያንስ\", \"ብ-ቅ\", \"ምሳሌ\", \"እንግዲ\", \"እሷ\", \"ምነው\", \"የተለያዩ\",\n",
        "    \"ወይስ\", \"እርስወት\", \"እንቶኔ\", \"እንቶኒት\", \"ኢ\", \"ኛ\", \"ነት\", \"በት\", \"ኤት\", \"ኤ\", \"ለይ\", \"ኦት\", \"ህ-ድ\", \"ዊ\",\n",
        "    \"እን\", \"ኧች-ች\", \"ሚ\", \"ኦች-ች\", \"ም-ት\", \"እዳ\", \"በየት\", \"ኤን\", \"ል-ይ\", \"ኢን\", \"ም-ን\", \"ስት\"\n",
        "]\n",
        "\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_imblearn\n",
        "\n",
        "class AdvancedStackingSentiment:\n",
        "    def __init__(self):\n",
        "        # Define base models with their respective pipelines\n",
        "        base_models = [\n",
        "            ('naive_bayes', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5, stop_words=amharic_stop_words),\n",
        "                SMOTE(),\n",
        "                MultinomialNB(alpha=0.1)\n",
        "            )),\n",
        "            ('svm', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5, stop_words=amharic_stop_words),\n",
        "                SMOTE(),\n",
        "                SVC(probability=True, kernel='linear', C=1)\n",
        "            )),\n",
        "            ('random_forest', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5, stop_words=amharic_stop_words),\n",
        "                SMOTE(),\n",
        "                RandomForestClassifier(n_estimators=100)\n",
        "            )),\n",
        "            ('log_reg', make_pipeline_imblearn(\n",
        "                TfidfVectorizer(ngram_range=(1, 7), analyzer='char', max_df=0.85, min_df=5, stop_words=amharic_stop_words),\n",
        "                SMOTE(),\n",
        "                LogisticRegression(C=1)\n",
        "            ))\n",
        "        ]\n",
        "\n",
        "        # Define the final estimator\n",
        "        final_estimator = LogisticRegression()\n",
        "\n",
        "        # Define the stacking classifier with cross-validation\n",
        "        self.pipeline = StackingClassifier(\n",
        "            estimators=base_models,\n",
        "            final_estimator=final_estimator,\n",
        "            cv=5\n",
        "        )\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.pipeline.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.pipeline.predict_proba(X)\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example data loading\n",
        "    # X_train, X_test, y_train, y_test should be initialized here\n",
        "    stacking_sentiment = AdvancedStackingSentiment()\n",
        "    stacking_sentiment.fit(X_train, y_train)\n",
        "    predictions = stacking_sentiment.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Accuracy of the stacking classifier: {accuracy:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}